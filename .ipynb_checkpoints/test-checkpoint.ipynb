{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9dd326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "232d1dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/workspace/notebooks/Carlos/Learning-Graph-Search-Heuristics-LGSH-')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0ca0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = pathlib.Path.cwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b39ce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:           /workspace/notebooks/Carlos/Learning-Graph-Search-Heuristics-LGSH-\n",
      "experiments:    /workspace/notebooks/Carlos/Learning-Graph-Search-Heuristics-LGSH-/experiments\n",
      "data:           /workspace/notebooks/Carlos/Learning-Graph-Search-Heuristics-LGSH-/data\n",
      "dataset:        /workspace/notebooks/Carlos/Learning-Graph-Search-Heuristics-LGSH-/data/motion_planning_datasets\n",
      "device:         cpu\n"
     ]
    }
   ],
   "source": [
    "root_path = pathlib.Path(root_dir)\n",
    "experiment_path = root_path / 'experiments'\n",
    "data_path = root_path / 'data'\n",
    "mpdataset_path = data_path / 'motion_planning_datasets'\n",
    "print('root:          ', root_path)\n",
    "print('experiments:   ', experiment_path)\n",
    "print('data:          ', data_path)\n",
    "print('dataset:       ', mpdataset_path)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device:        ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b11b735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_pytorch_version(version):\n",
    "  return version.split('+')[0]\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "TORCH = format_pytorch_version(TORCH_version)\n",
    "\n",
    "def format_cuda_version(version):\n",
    "  return 'cu' + version.replace('.', '')\n",
    "\n",
    "CUDA_version = torch.version.cuda\n",
    "CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
    "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
    "# !pip install torch-cluster -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
    "# !pip install torch-spline-conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
    "# !pip install torch-geometric\n",
    "# !pip install dill\n",
    "#!pip install torch_geometric\n",
    "#dill\n",
    "#imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b35059fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "\n",
    "from torch.nn import GRU\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from torch_geometric.utils.convert import from_networkx, to_networkx\n",
    "from torch_geometric.nn.conv import GENConv\n",
    "from torch_geometric.nn.models import Node2Vec\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.transforms import normalize_features\n",
    "\n",
    "# Standard library\n",
    "from copy import copy, deepcopy\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Any, Callable, Iterator, Optional, Protocol, TypedDict, Union\n",
    "import gc\n",
    "import heapq\n",
    "import itertools\n",
    "import pathlib\n",
    "# import pickle\n",
    "import threading\n",
    "\n",
    "# Third party libraries\n",
    "from IPython import display\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import dill as pickle # better pickle\n",
    "import imageio.v3 as iio\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dee36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For proper intellisense\n",
    "class Parameters(TypedDict):\n",
    "    # GCN\n",
    "    k_hop_neighborhood: int\n",
    "    # GRU\n",
    "    memory_embedding_dim: int\n",
    "    num_memory_layers: int\n",
    "    # MLP\n",
    "    mlp_hidden_dim: int\n",
    "    # Mixture policy / OracleControlelr\n",
    "    beta: float\n",
    "    # A* Runner\n",
    "    max_neighborhood_size: int # n in the paper\n",
    "    trajectory_length: int # T in the paper\n",
    "    rollout_length: int # t_tau in the paper\n",
    "    # Learner\n",
    "    learning_rate: float\n",
    "    # Experiment\n",
    "    max_episodes: int # N in the paper\n",
    "    num_trajectories: int # m in the paper\n",
    "    patience: int # Early stopping\n",
    "    patience_margin: int # Early stopping\n",
    "\n",
    "\n",
    "def default_parameters() -> Parameters:\n",
    "    return Parameters({\n",
    "        # GCN\n",
    "        'k_hop_neighborhood': 1, # From the PHIL paper section 4.1 on \"Recurrent GNN Architecture\"\n",
    "        # GRU\n",
    "        'memory_embedding_dim': 64, # From the PHIL paper section 5.1\n",
    "        'num_memory_layers': 1, # Arbibrary choice, pytorch default is 1, not stated in the paper.\n",
    "        # MLP\n",
    "        'mlp_hidden_dim': 128, # From the PHIL paper section 5.1\n",
    "        # Mixture policy / OracleControlelr\n",
    "        'beta': 0.7, # From the PHIL paper section 5.1\n",
    "        # A* Runner\n",
    "        'max_neighborhood_size': 8, # From the PHIL paper section 5.1\n",
    "        'trajectory_length': 128, # From the PHIL paper section 5.1\n",
    "        'rollout_length': 32, # From the PHIL paper section 5.1\n",
    "        # Learner\n",
    "        'learning_rate': 1e-3, # From appendix C on optimization.\n",
    "        # Experiment\n",
    "        'max_episodes': 40, # section 5.1 states convergence after 36/200 iterations\n",
    "        'num_trajectories': 1, # From the PHIL paper section 5.1\n",
    "        'patience': 5,\n",
    "        'patience_margin': 50,\n",
    "    })\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "910e7e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create root folder\n",
    "if not root_path.exists():\n",
    "    print('created root folder')\n",
    "    root_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Populate root folder with raw and processed data from data.zip\n",
    "if root_path.exists() and pathlib.Path('data.zip').exists():\n",
    "    print('Root path exists and data.zip exists')\n",
    "    !unzip 'data.zip'\n",
    "\n",
    "# Alternatively populate only with raw data.\n",
    "if not mpdataset_path.exists() and pathlib.Path('motion_planning_datasets-master.zip').exists():\n",
    "    print('populate with raw data')\n",
    "    !unzip 'motion_planning_datasets-master.zip' -d 'data'\n",
    "    (root_path / 'motion_planning_datasets-master').rename(mpdataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bc9cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip -r 'data.zip' 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d52ac5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def node_name_from_pos(yx) -> Tuple[int, int]:\n",
    "    return yx[0], yx[1]\n",
    "\n",
    "def nodes_from_img(img: np.ndarray) -> List[Tuple[Tuple[int, int], dict]]:\n",
    "    #Nodes and their x, y coordinates are read from an image\n",
    "    # where white pixels correspond to nodes.\n",
    "    return [(node_name_from_pos(yx), {'ypos': yx[0], 'xpos': yx[1]}) for yx in np.argwhere(img)]\n",
    "\n",
    "def edges_from_img(img: np.ndarray) -> List[Tuple[Tuple[int, int], Tuple[int, int]]]:\n",
    "    # Get all source pixels with vertical transitions from and to white pixels\n",
    "    # from top to bottom and not wrapping around.\n",
    "    vertical_sources = np.argwhere(img[:-1] & img[1:])\n",
    "    vertical_targets = vertical_sources + (1, 0)\n",
    "\n",
    "    # Same trick horizontally, but now left to right\n",
    "    horizontal_sources = np.argwhere(img[:,:-1] & img[:,1:])\n",
    "    horizontal_targets = horizontal_sources + (0, 1)\n",
    "\n",
    "    # Edges go by node name (source, target)\n",
    "    edges = []\n",
    "    for source, target in zip(\n",
    "        [tuple(yx) for yx in np.vstack((vertical_sources, horizontal_sources))],\n",
    "        [tuple(yx) for yx in np.vstack((vertical_targets, horizontal_targets))]\n",
    "    ):\n",
    "        edges.append((node_name_from_pos(source), node_name_from_pos(target)))\n",
    "    return edges\n",
    "\n",
    "def goal_node_from_nx(graph: nx.Graph) -> Tuple[int, int]:\n",
    "    # Goal node corresponds to the top-right pixel, assuming the pixel is white.\n",
    "    # Top-right pixel will have y=0 and x=width-1\n",
    "    max_x = -1\n",
    "    for _, data in graph.nodes(data='xpos'):\n",
    "        if data is not None and data > max_x:\n",
    "            max_x = data\n",
    "    return 0, max_x\n",
    "\n",
    "def source_node_from_nx(graph: nx.Graph) -> Tuple[int, int]:\n",
    "    # Source node corresponds to the bottom-left pixel, assuming the pixel is white.\n",
    "    # Bottom-left pixel will have y=height-1 and x=0\n",
    "    max_y = -1\n",
    "    for _, data in graph.nodes(data='ypos'):\n",
    "        if data is not None and data > max_y:\n",
    "            max_y = data\n",
    "    return max_y, 0\n",
    "\n",
    "def image_to_nx(img: np.ndarray) -> nx.Graph:\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(nodes_from_img(img))\n",
    "    graph.add_edges_from(edges_from_img(img))\n",
    "\n",
    "    ### Node attributes\n",
    "    # degrees = dict(graph.degree())\n",
    "    # nx.set_node_attributes(graph, degrees, 'degree')\n",
    "\n",
    "    # Precompute the shortest paths to the goal.\n",
    "    goal_node = goal_node_from_nx(graph)\n",
    "    try:\n",
    "        shortest_path_lengths_to_goal = nx.shortest_path_length(graph, target=goal_node)\n",
    "        nx.set_node_attributes(graph, shortest_path_lengths_to_goal, 'y')\n",
    "\n",
    "        # Remove nodes which are disconnected from the goal.\n",
    "        # These will not have a 'y' attribute set by the code above.\n",
    "        # Not removing this will result in a non-homogeneous graph i.e. bad for pytorch data.\n",
    "        graph.remove_nodes_from([node for node, y in graph.nodes(data='y') if y is None])\n",
    "    except nx.NetworkXNoPath:\n",
    "        print(f\"Warning: No path to the goal node {goal_node} exists in the graph.\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "# Ejemplo de uso (puedes descomentar para probar):\n",
    "# img = np.array([[True, True], [True, False]])\n",
    "# graph = image_to_nx(img)\n",
    "# print(\"Nodos del grafo:\", graph.nodes(data=True))\n",
    "# print(\"Aristas del grafo:\", graph.edges())\n",
    "# print(\"Nodo objetivo:\", goal_node_from_nx(graph))\n",
    "# print(\"Nodo fuente:\", source_node_from_nx(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f485e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(Enum):\n",
    "    ALTERNATING_GAPS  = 'alternating_gaps'\n",
    "    BUGTRAP_FOREST    = 'bugtrap_forest'\n",
    "    FOREST            = 'forest'\n",
    "    GAPS_AND_FOREST   = 'gaps_and_forest'\n",
    "    MAZES             = 'mazes'\n",
    "    MULTIPLE_BUGTRAPS = 'multiple_bugtraps'\n",
    "    SHIFTING_GAPS     = 'shifting_gaps'\n",
    "    SINGLE_BUGTRAP    = 'single_bugtrap'\n",
    "\n",
    "class MotionPlanningDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None, name=Datasets.ALTERNATING_GAPS, split='train'):\n",
    "        dir = pathlib.Path(root, name.value, split)\n",
    "        assert dir.exists(), f'invalid directory: {dir}'\n",
    "\n",
    "        self.root = str(dir.resolve())\n",
    "        self._processed_dir = str(pathlib.Path(f'{root}_processed', name.value, split).resolve())\n",
    "        self._raw_files = []\n",
    "        self._processed_files = []\n",
    "        self._load_file_names()\n",
    "\n",
    "        super().__init__(self.root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    def _load_file_names(self):\n",
    "        \"\"\"Folder structure is as follows:\n",
    "        root/motion_planning_dataset/bugtrap_forest/train\"\"\"\n",
    "        paths = (entry.resolve() for entry in pathlib.Path(self.root).iterdir()\n",
    "                 if entry.is_file())\n",
    "\n",
    "        # Store only the filenames, not the full paths.\n",
    "        for path in paths:\n",
    "            self._raw_files.append(str(path.name))\n",
    "            self._processed_files.append(str(path.with_suffix('.pt').name))\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return self._raw_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return self._processed_files\n",
    "    \n",
    "    @property\n",
    "    def raw_dir(self):\n",
    "        return self.root\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self):\n",
    "        return self._processed_dir\n",
    "\n",
    "    def process(self):\n",
    "        for (raw_path, processed_path) in zip(self.raw_paths, self.processed_paths):\n",
    "            # Convert 8-bit color image to 1-bit black and white image\n",
    "            img = iio.imread(raw_path) > 0\n",
    "            graph = image_to_nx(img)\n",
    "            # Concatenate both xpos and ypos into one attribute data.x\n",
    "            # data.x contains node features\n",
    "            # data.y contains shortest distance to top right / goal node.\n",
    "            data = from_networkx(graph, group_node_attrs=['xpos', 'ypos'])\n",
    "\n",
    "            if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                continue\n",
    "\n",
    "            if self.pre_transform is not None:\n",
    "                data = self.pre_transform(data)\n",
    "\n",
    "            torch.save(data, processed_path)\n",
    "            gc.collect()\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx: int) -> torch_geometric.data.Data:\n",
    "        data = torch.load(self.processed_paths[idx])\n",
    "        return data\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd4e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    }
   ],
   "source": [
    "# Data from img_to_nx gives integers\n",
    "def cast_to_float(data: Data):\n",
    "    data.x = data.x.to(torch.float32)\n",
    "    data.y = data.y.to(torch.float32)\n",
    "    return data\n",
    "\n",
    "alternating_gaps_train = MotionPlanningDataset(mpdataset_path, name=Datasets.ALTERNATING_GAPS, pre_transform=cast_to_float, split='train')\n",
    "alternating_gaps_validation = MotionPlanningDataset(mpdataset_path, name=Datasets.ALTERNATING_GAPS, pre_transform=cast_to_float, split='validation')\n",
    "alternating_gaps_test = MotionPlanningDataset(mpdataset_path, name=Datasets.ALTERNATING_GAPS, pre_transform=cast_to_float, split='test')\n",
    "\n",
    "bugtrap_forest_train = MotionPlanningDataset(mpdataset_path, name=Datasets.BUGTRAP_FOREST, pre_transform=cast_to_float, split='train')\n",
    "bugtrap_forest_validation = MotionPlanningDataset(mpdataset_path, name=Datasets.BUGTRAP_FOREST, pre_transform=cast_to_float, split='validation')\n",
    "bugtrap_forest_test = MotionPlanningDataset(mpdataset_path, name=Datasets.BUGTRAP_FOREST, pre_transform=cast_to_float, split='test')\n",
    "\n",
    "gaps_and_forest_train = MotionPlanningDataset(mpdataset_path, name=Datasets.GAPS_AND_FOREST, pre_transform=cast_to_float, split='train')\n",
    "gaps_and_forest_validation = MotionPlanningDataset(mpdataset_path, name=Datasets.GAPS_AND_FOREST, pre_transform=cast_to_float, split='validation')\n",
    "gaps_and_forest_test = MotionPlanningDataset(mpdataset_path, name=Datasets.GAPS_AND_FOREST, pre_transform=cast_to_float, split='test')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeuristicFunction(torch.nn.Module):\n",
    "    \"\"\"Implements Algorithm 2 from the paper.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_node_features: int,\n",
    "        params: Parameters,\n",
    "        # node_feature_encoder = None,\n",
    "        node_embedder = None,\n",
    "        gcn = None,\n",
    "        gru = None,\n",
    "        mlp = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Network parameters\n",
    "        self._num_node_features = num_node_features\n",
    "        # x_i features + x_goal features + dist_euclidean + dist_cosine.\n",
    "        self._num_embedded_node_features = 2 * num_node_features + 2\n",
    "        # GCN\n",
    "        self._k_hop_neighborhood = params['k_hop_neighborhood']\n",
    "        # GRU\n",
    "        self._memory_embedding_dim = params['memory_embedding_dim']\n",
    "        self._num_memory_layers = params['num_memory_layers']\n",
    "        # MLP\n",
    "        self._mlp_hidden_dim = params['mlp_hidden_dim']\n",
    "\n",
    "        # Network modules\n",
    "        # self._node_feature_encoder = node_feature_encoder\n",
    "        self._node_embedder = node_embedder\n",
    "        self._gcn = gcn\n",
    "        self._gru = gru\n",
    "        self._mlp = mlp\n",
    "\n",
    "        # Use torch_geometric.nn.Node2Vec\n",
    "        # Does not work as the edge_index changes per graph.\n",
    "        # if node_feature_encoder is None:\n",
    "        #     self._node_feature_encoder = Node2Vec(\n",
    "        #         edge_index,\n",
    "        #         embedding_dim=num_node_features,\n",
    "        #         walk_length=walk_length,\n",
    "        #         context_size=context_size,\n",
    "        #         walks_per_node=walks_per_node\n",
    "        #     )\n",
    "\n",
    "        # Shallow network\n",
    "        if node_embedder is None:\n",
    "            self._node_embedder = torch.nn.Linear(\n",
    "                self._num_embedded_node_features,\n",
    "                self._num_embedded_node_features,\n",
    "            )\n",
    "\n",
    "        # DeeperGCN\n",
    "        if gcn is None:\n",
    "            self._gcn = GENConv(\n",
    "                in_channels=self._num_embedded_node_features,\n",
    "                out_channels=self._num_embedded_node_features,\n",
    "                aggr='softmax',\n",
    "            )\n",
    "        \n",
    "        # GRU Notes:\n",
    "        # Nothing is said about the number of hidden layers\n",
    "        # The paper also does not mention how it takes the \"sample mean\"\n",
    "        # over the memory state. Taking the sample mean is impossible without \n",
    "        # changing the shape of the memory state. Batching and then taking the mean\n",
    "        # is also not possible since we then also have to give a batched input memory.\n",
    "        # The only way to hack it is by duplicating the memory state to the correct batch size\n",
    "        # then doing a batched GRU pass and calculating the mean of the batch.\n",
    "        if gru is None:\n",
    "            self._gru = GRU(\n",
    "                input_size=self._num_embedded_node_features,\n",
    "                hidden_size=self._memory_embedding_dim,\n",
    "                num_layers=self._num_memory_layers,\n",
    "            )\n",
    "\n",
    "        # Multi Layer Perceptron\n",
    "        if mlp is None:\n",
    "            self._mlp = torch.nn.Sequential(\n",
    "                # x_goal features + g' features.\n",
    "                torch.nn.Linear(self._num_node_features + self._memory_embedding_dim, self._mlp_hidden_dim), torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self._mlp_hidden_dim, self._mlp_hidden_dim), torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self._mlp_hidden_dim, self._mlp_hidden_dim), torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self._mlp_hidden_dim, self._mlp_hidden_dim), torch.nn.LeakyReLU(),\n",
    "                # Output layer - outputs the scalar distance metric\n",
    "                torch.nn.Linear(self._mlp_hidden_dim, 1),\n",
    "            )\n",
    "\n",
    "\n",
    "    def _get_encoded_node_features(self, x: torch.Tensor, goal: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the encoded node features as well as the euclidean and cosine distances.\"\"\"\n",
    "        # Keepdim and unsqueeze make the output shape (N) -> (N,1)\n",
    "        # so that we can concatenate them along dim 1 in the return value.\n",
    "        dist_euclidean = F.pairwise_distance(x, goal, p=2).unsqueeze(dim=1)\n",
    "        dist_cosine = F.cosine_similarity(x, goal, dim=1).unsqueeze(dim=1)\n",
    "\n",
    "        # Use cat, not stack as stack creates an extra dimension.\n",
    "        # Concatenate the features width wise (N, D1) + (N, D2) = (N, D1 + D2)\n",
    "        return torch.cat((x, goal, dist_euclidean, dist_cosine), dim=1)\n",
    "\n",
    "    def generate_memory(self, graph=None) -> torch.Tensor:\n",
    "        \"\"\"Generates randomly initialized memory.\n",
    "        See: https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\"\"\"\n",
    "        # return torch.randn((self._num_memory_layers, graph.x.shape[0], self._memory_embedding_dim))\n",
    "        return torch.randn((self._num_memory_layers, self._memory_embedding_dim))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        graph: Data,\n",
    "        node_index: torch.LongTensor,\n",
    "        goal_index: torch.LongTensor,\n",
    "        memory: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Steps:\n",
    "        - Project nodes all into embedding space using embed_node(x_i, x_g, D_euc, D_cos)\n",
    "        - Obtain g_i by doing a single graph convolution on x_i\n",
    "        - Obtain hidden states z_i,t+1 and new embedding g'_i using GRU(g_i, z_t)\n",
    "        - Take mean over the nodes in node_index in z_i,t+1 to get z_t+1\n",
    "        - Get distance metric h from MLP(g'_i, x_g) and using the node index to get the actual nodes.\n",
    "        \"\"\"\n",
    "        # Repeat the goal index for every node in the graph.\n",
    "        goal_index = goal_index.repeat(graph.num_nodes)\n",
    "        # We encode all nodes and then use the edge_index to define the 1-hop neighborhood\n",
    "        x = self._get_encoded_node_features(graph.x, graph.x[goal_index].clone())\n",
    "        x = self._node_embedder(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        # Perform k-hop neighborhood aggregation.\n",
    "        for _ in range(self._k_hop_neighborhood):\n",
    "            x = self._gcn(x, graph.edge_index)\n",
    "            x = F.leaky_relu(x)\n",
    "\n",
    "        x = x[node_index]\n",
    "        # To be able to take the sample mean over the gru, we have to \"batch\"\n",
    "        # the input so we can then take the mean over that batch dimension,\n",
    "        # once again giving use the correct memory shape.\n",
    "        # Shape (D, H_out) -> (D, 1, H_out) -> (D, N, H_out)\n",
    "        memory = memory.unsqueeze(1).repeat((1, node_index.shape[0], 1))\n",
    "        x, memory = self._gru(x.unsqueeze(0), memory)\n",
    "        x = F.leaky_relu(x.squeeze(0))\n",
    "        # Take the mean over the nodes in the open set\n",
    "        memory = torch.mean(memory, dim=1)\n",
    "\n",
    "        # Concatenate the features width-wise: (N, D1) + (N, D2) -> (N, D1 + D2)\n",
    "        x_goal = graph.x[goal_index[node_index]].clone()\n",
    "        h = self._mlp(torch.cat((x, x_goal), dim=1))\n",
    "        h = F.leaky_relu(h)\n",
    "\n",
    "        return h, memory\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e52ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class euclidean_distance_func():\n",
    "  def __init__(self):\n",
    "    # Nothing to initialize so...\n",
    "    pass\n",
    "\n",
    "  def __call__(self, graph: Data, node_index: torch.LongTensor, goal_index: torch.LongTensor, memory) -> tuple([list, None]):\n",
    "    \"\"\"Given the graph and the corresponding fringe_set, returns the euclidean\n",
    "    distances between the nodes and the target node.\n",
    "    \n",
    "    Memory is going to remain empty for now? It's just a leftover value from other\n",
    "    stuff, that isn't really relevant for our needs\"\"\"\n",
    "\n",
    "    goal_index = goal_index.repeat(graph.num_nodes)\n",
    "\n",
    "    x = torch.norm(graph.x[node_index] - graph.x[goal_index], 2)\n",
    "    x_goal = graph.x[goal_index[node_index]].clone()\n",
    "\n",
    "    h = torch.cat((x, x_goal), dim=1)\n",
    "\n",
    "    return h, None\n",
    "\n",
    "  def eval() -> None:\n",
    "    # Leftover function for compatability with other stuff\n",
    "    pass\n",
    "\n",
    "  def generate_memory(self, graph=None) -> torch.Tensor:\n",
    "    # Leftover stuff for compatibility!\n",
    "    return torch.randn((4, 2))\n",
    "\n",
    "#heuristic_func(self.graph, fringe_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e80f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order creates comparison dunder methods\n",
    "@dataclass(order=True)\n",
    "class PrioritizedItem:\n",
    "    priority: float\n",
    "    item: Any=field(compare=False)\n",
    "\n",
    "class PriorityQueue:\n",
    "    def __init__(self):\n",
    "        self._priority_queue: list[PrioritizedItem] = []\n",
    "        self._entry_finder: dict[Any, PrioritizedItem] = {}\n",
    "\n",
    "    def enqueue(self, item: Any, priority=0) -> None:\n",
    "        \"\"\"Add a new entry or update the priority of an existing entry\"\"\"\n",
    "        if item in self._entry_finder:\n",
    "            self.remove(item)\n",
    "\n",
    "        entry = PrioritizedItem(priority, item)\n",
    "        self._entry_finder[item] = entry\n",
    "        heapq.heappush(self._priority_queue, entry)\n",
    "\n",
    "    def dequeue(self) -> Any:\n",
    "        \"\"\"Remove and return the lowest priority item. Raise KeyError if empty.\"\"\"\n",
    "        while self._priority_queue:\n",
    "            entry = heapq.heappop(self._priority_queue)\n",
    "\n",
    "            if entry.item is not None:\n",
    "                del self._entry_finder[entry.item]\n",
    "                return entry.item\n",
    "        \n",
    "        raise KeyError('pop from an empty priority queue')\n",
    "\n",
    "    def remove(self, item: Any) -> None:\n",
    "        \"\"\"Mark an existing entry as removed.\"\"\"\n",
    "        entry = self._entry_finder.pop(item)\n",
    "        entry.item = None   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectory:\n",
    "    fringe_sets: torch.LongTensor\n",
    "    rollin_memory: torch.Tensor\n",
    "\n",
    "    def __init__(self, fringe_size: int, rollin_memory, device='cpu'):\n",
    "        self._lock = threading.Lock()\n",
    "        self.rollin_memory = rollin_memory\n",
    "        self.fringe_sets = []\n",
    "        self._device = device\n",
    "\n",
    "    # For pickling, as threading.lock is unpickleable.\n",
    "    def __getstate__(self):\n",
    "        return {\n",
    "            'rollin_memory': self.rollin_memory,\n",
    "            'fringe_sets': self.fringe_sets,\n",
    "            'device': self._device\n",
    "        }\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.rollin_memory = state['rollin_memory']\n",
    "        self.fringe_sets = state['fringe_sets']\n",
    "        self._device = state['device']\n",
    "        self._lock = threading.Lock()\n",
    "\n",
    "    def _diff_node_indices(self, fringe_set: torch.LongTensor, prev_fringe_set: torch.LongTensor) -> torch.LongTensor:\n",
    "        \"\"\"Returns the set difference of two index tensors, fringe_set - prev_node_index.\"\"\"\n",
    "        mask = torch.zeros((max(fringe_set.max().item(), prev_fringe_set.max().item()) + 1), dtype=torch.bool)\n",
    "        mask[fringe_set] = True\n",
    "        mask[prev_fringe_set] = False\n",
    "        return mask.nonzero().flatten()\n",
    "    \n",
    "    def add(self, fringe_set: torch.LongTensor):\n",
    "        \"\"\"Adds the new fringe set to the trajectory and automatically\n",
    "        calculates the difference with the previous fringe set.\"\"\"\n",
    "        self._lock.acquire()\n",
    "        try:\n",
    "            if fringe_set.numel() == 0:\n",
    "                return self\n",
    "\n",
    "            # Take the difference of the two fringe sets as stated in the paper\n",
    "            if len(self.fringe_sets) > 0:\n",
    "                prev_fringe_set = self.fringe_sets[-1]\n",
    "                fringe_set = self._diff_node_indices(fringe_set, prev_fringe_set)\n",
    "\n",
    "            self.fringe_sets.append(fringe_set.to(self._device))\n",
    "        finally: self._lock.release()\n",
    "        return self\n",
    "    \n",
    "\n",
    "class TrajectoryDatabase:\n",
    "    def __init__(self):\n",
    "        self._lock = threading.Lock()\n",
    "        self._size = 0\n",
    "        self._indices = [] # For iterating over the database.\n",
    "        self._dict: dict[int, list[Trajectory]] = {}\n",
    "\n",
    "    # For pickling, as threading.lock is unpickleable.\n",
    "    def __getstate__(self):\n",
    "        return {\n",
    "            'size': self._size,\n",
    "            'indices': self._indices,\n",
    "            'dict': self._dict\n",
    "        }\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self._size = state['size']\n",
    "        self._indices = state['indices']\n",
    "        self._dict = state['dict']\n",
    "        print('self state', state)\n",
    "        self._lock = threading.Lock()\n",
    "\n",
    "    @property\n",
    "    def dict(self):\n",
    "        return self._dict\n",
    "    \n",
    "    def add(self, graph_id: int, trajectory: Trajectory):\n",
    "        \"\"\"Add a trajectory to a graph.\"\"\"\n",
    "        self._lock.acquire()\n",
    "        \n",
    "        try:\n",
    "            self._dict[graph_id] = self._dict.get(graph_id, [])\n",
    "            self._dict[graph_id].append(trajectory)\n",
    "            self._size += 1\n",
    "        finally: self._lock.release()\n",
    "        return self\n",
    "\n",
    "    def __len__(self): \n",
    "        \"\"\"Returns the length of the buffer.\"\"\"\n",
    "        return self._size\n",
    "    \n",
    "    def __iter__(self):  \n",
    "        \"\"\"Initializes an iterator over the buffer.\"\"\"\n",
    "        self._indices = list(self._dict.keys())\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):  \n",
    "        \"\"\"Iterates through the graphs in the database, returns list of trajectories.\"\"\"\n",
    "        if len(self._indices) == 0: raise StopIteration\n",
    "        graph_id = self._indices[-1]\n",
    "        self._indices = self._indices[:-1]\n",
    "        return graph_id, self._dict[graph_id]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b9b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GraphInformation:\n",
    "    graph: Data\n",
    "    source_index: torch.LongTensor\n",
    "    goal_index: torch.LongTensor\n",
    "\n",
    "def sample_source_node(graph: Data, random=False, device='cpu') -> torch.LongTensor:\n",
    "    \"\"\"Samples a source node from the graph.\n",
    "    if random=False then the bottom left node is chosen.\"\"\"\n",
    "    \n",
    "    if random:\n",
    "        return torch.tensor([np.random.randint(graph.x.shape[0])], dtype=torch.long, device=device)\n",
    "\n",
    "    # Compute the minimum value of the x coordinate\n",
    "    # and the maximum value of the y coordinate\n",
    "    # to get the bottom-left corner of the graph.\n",
    "    min_x = torch.min(graph.x[:, 0])\n",
    "    max_y = torch.max(graph.x[:, 1])\n",
    "\n",
    "    # Get the indices of nodes that have the maximum x coordinate and the minimum y coordinate\n",
    "    min_x_indices = (graph.x[:, 0] == min_x).nonzero(as_tuple=True)[0].tolist()\n",
    "    max_y_indices = (graph.x[:, 1] == max_y).nonzero(as_tuple=True)[0].tolist()\n",
    "\n",
    "    # Return the first item of intersection of the two sets of indices\n",
    "    return torch.as_tensor(list(set(min_x_indices) & set(max_y_indices))[:1], dtype=torch.long, device=device)\n",
    "\n",
    "def sample_goal_node(graph: Data, device='cpu') -> torch.LongTensor:\n",
    "    \"\"\"Gets the top-right node from the graph.\"\"\"\n",
    "    # Compute the maximum value of the x coordinate\n",
    "    # and the minimum value of the y coordinate\n",
    "    # to get the top-right corner of the graph.\n",
    "    max_x = torch.max(graph.x[:, 0])\n",
    "    min_y = torch.min(graph.x[:, 1])\n",
    "\n",
    "    # Get the indices of nodes that have the maximum x coordinate and the minimum y coordinate\n",
    "    max_x_indices = (graph.x[:, 0] == max_x).nonzero(as_tuple=True)[0].tolist()\n",
    "    min_y_indices = (graph.x[:, 1] == min_y).nonzero(as_tuple=True)[0].tolist()\n",
    "\n",
    "    # Return the first item of intersection of the two sets of indices\n",
    "    return torch.as_tensor(list(set(max_x_indices) & set(min_y_indices))[:1], dtype=torch.long, device=device)\n",
    "\n",
    "def sample_graph_information(graph: Data, random=False, device='cpu') -> GraphInformation:\n",
    "    # Source and goal node have to be found before normalization, or else\n",
    "    #  the normalization screws up the definition of what it should be\n",
    "    source_node = sample_source_node(graph, random=random, device=device)\n",
    "    goal_node = sample_goal_node(graph, device=device)\n",
    "    graph = normalize_features.NormalizeFeatures()(graph.clone()).to(device)\n",
    "    return GraphInformation(\n",
    "        graph,\n",
    "        source_node,\n",
    "        goal_node,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de644ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AStarEnvironment:\n",
    "    def __init__(self, heuristics: dict[str, Callable], params: Parameters):\n",
    "        \"\"\"An A* environment containing a variable amount of heuristic functions to use.\n",
    "        heuristics: dict[name, heuristic(graph, node_index) -> torch.Tensor]\n",
    "        \"\"\"\n",
    "        self._heuristics = heuristics\n",
    "        self._max_neighborhood_size = params['max_neighborhood_size']\n",
    "\n",
    "    def reset(self, graph: Data, source_index: torch.LongTensor, goal_index: torch.LongTensor):\n",
    "        self.graph = graph\n",
    "        self.source_index = source_index\n",
    "        self.goal_index = goal_index\n",
    "        self._queues = {name: PriorityQueue() for name in self._heuristics.keys()}\n",
    "        self._g_scores = {name: {} for name in self._heuristics.keys()}\n",
    "        self._f_scores = {name: {} for name in self._heuristics.keys()}\n",
    "        self.closed_set = set()\n",
    "\n",
    "        # Seed the environment with an initial value.\n",
    "        for name, heuristic_func in self._heuristics.items():\n",
    "            # Index tensor value is used as priority queue hash\n",
    "            hash = source_index.item()\n",
    "            f_score = heuristic_func(graph, source_index)\n",
    "            self._g_scores[name][hash] = 0\n",
    "            self._f_scores[name][hash] = f_score\n",
    "            self._queues[name].enqueue(hash, f_score)\n",
    "\n",
    "    def _get_node_by_heuristic(self, heuristic: str) -> int:\n",
    "        \"\"\"Pops one item from the specified queues and removes\n",
    "        it from the other queues.\n",
    "        \n",
    "        Returns: the node index stored inside.\"\"\"\n",
    "        node_index = self._queues[heuristic].dequeue()\n",
    "\n",
    "        for name, queue in self._queues.items():\n",
    "            if name == heuristic:\n",
    "                continue\n",
    "\n",
    "            queue.remove(node_index)\n",
    "\n",
    "        return node_index\n",
    "\n",
    "    def _get_open_neighbors(self, node_index: int) -> torch.LongTensor:\n",
    "        neighborhood_index = k_hop_subgraph(node_index, 1, self.graph.edge_index)[0]\n",
    "        # Select the open nodes from the neighborhood\n",
    "        fringe_set_mask = [neighbor.item() not in self.closed_set\n",
    "                           for neighbor in neighborhood_index]\n",
    "        fringe_set = neighborhood_index[fringe_set_mask]\n",
    "        # Uniformly select at most n nodes from the neighborhood.\n",
    "        random_mask = torch.randperm(fringe_set.shape[0])\n",
    "        random_mask = random_mask[:self._max_neighborhood_size]\n",
    "        return fringe_set[random_mask]\n",
    "\n",
    "    def run(self, heuristic) -> tuple[torch.LongTensor, bool]:\n",
    "        \"\"\"Runs one step of the environment and updates internal bookkeeping.\n",
    "        \n",
    "        Returns: fringe set, done\"\"\"\n",
    "        node_index = self._get_node_by_heuristic(heuristic)\n",
    "        self.closed_set.add(node_index)\n",
    "        fringe_set = self._get_open_neighbors(node_index)\n",
    "\n",
    "        # Found the goal node\n",
    "        if node_index == self.goal_index.item():\n",
    "            return fringe_set, True\n",
    "\n",
    "        if len(fringe_set) == 0:\n",
    "            return fringe_set, False\n",
    "\n",
    "        for name, heuristic_func in self._heuristics.items():\n",
    "            # Get datastructures corresponding to current heuristic.\n",
    "            heuristics = heuristic_func(self.graph, fringe_set)\n",
    "            g_scores = self._g_scores[name]\n",
    "            f_scores = self._f_scores[name]\n",
    "            queue = self._queues[name]\n",
    "            \n",
    "            # Add open nodes according to A* rules.\n",
    "            for index, heuristic in zip(fringe_set, heuristics):\n",
    "                # Euclidean distance seems most popular for distance between nodes.\n",
    "                dist = torch.norm(self.graph.x[node_index] - self.graph.x[index], 2).item()\n",
    "                # Calculate new scores\n",
    "                g_score = g_scores[node_index] + dist\n",
    "                f_score = g_score + heuristic.item()\n",
    "\n",
    "                # We cannot use tensors as hash, as they are hashed by id not value.\n",
    "                hash = index.item()\n",
    "\n",
    "                # Update state for heuristic queue\n",
    "                if hash not in g_scores or g_score < g_scores[hash]:\n",
    "                    self._g_scores[name][hash] = g_score\n",
    "                    self._f_scores[name][hash] = f_score\n",
    "                    self._queues[name].enqueue(hash, f_score)\n",
    "\n",
    "        return fringe_set, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c92c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(Protocol):\n",
    "    \"\"\"A controller selects an action for the runner\n",
    "    and handles all the interations with the model.\"\"\"\n",
    "    def choose(self, **kwargs) -> Any:\n",
    "        ...\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d826a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OracleController:\n",
    "    def __init__(self, model, actions: list[str], params: Parameters, device='cpu'):\n",
    "        \"\"\"Selects the mixture policy of PHIL during training.\n",
    "    and handles all the interations with the model.\"\"\"\n",
    "        self._beta = params['beta']\n",
    "        self._model = model\n",
    "        self._heuristic = actions[0]\n",
    "        self._oracle = actions[1]\n",
    "        self._device = device\n",
    "\n",
    "    # Used in the A* environment\n",
    "    @property\n",
    "    def heuristics(self):\n",
    "        return {\n",
    "            self._heuristic: self.heuristic,\n",
    "            self._oracle: self.oracle,\n",
    "        }\n",
    "\n",
    "    def reset(self, goal_index):\n",
    "        self._goal_index = goal_index\n",
    "        self.memory = self._model.generate_memory().to(self._device)\n",
    "\n",
    "    def set_beta(self, beta: float) -> None:\n",
    "        self._beta = beta\n",
    "\n",
    "    def heuristic(self, graph: Data, node_index: torch.LongTensor) -> torch.FloatTensor:\n",
    "        heuristic, self.memory = self._model(graph, node_index, self._goal_index, self.memory)\n",
    "        return heuristic\n",
    "\n",
    "    def oracle(self, graph: Data, node_index: torch.LongTensor) -> torch.FloatTensor:\n",
    "        return graph.y[node_index]\n",
    "\n",
    "    def choose(self, rollin=False) -> Any:\n",
    "        \"\"\"Executes the mixture policy by stochastically choosing\n",
    "        between the two heuristics.\"\"\"\n",
    "        if rollin or np.random.rand() > self._beta:\n",
    "            return self._heuristic\n",
    "\n",
    "        return self._oracle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a065f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Runner(Protocol):\n",
    "    def run(self) -> dict:\n",
    "        ...\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825240ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AStarRunner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        controller: Controller,\n",
    "        params: Parameters,\n",
    "        device='cpu',\n",
    "    ):\n",
    "        \"\"\"A single threaded runner for the A* environment.\"\"\"\n",
    "        self._controller = controller\n",
    "        self._env = AStarEnvironment(controller.heuristics, params)\n",
    "        self._device = device\n",
    "\n",
    "        # Hyper parameters\n",
    "        self._max_neighborhood_size = params['max_neighborhood_size']\n",
    "        self._trajectory_length = params['trajectory_length']\n",
    "        self._rollout_length = params['rollout_length']\n",
    "\n",
    "    def _reset_state(self, graph: GraphInformation):\n",
    "        # Release resources as we do not need the current memory anymore.\n",
    "        self._controller.reset(graph.goal_index)\n",
    "        self._env.reset(graph.graph, graph.source_index, graph.goal_index)\n",
    "\n",
    "    def _run_step(self, rollin=False) -> tuple[torch.LongTensor, bool]:\n",
    "        \"\"\"Runs a single step of the A* algorithm while updating the environment.\n",
    "        Returns the new fringe mask, and whether the environment is done.\"\"\"\n",
    "        heuristic = self._controller.choose(rollin=rollin)\n",
    "        return self._env.run(heuristic)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def run(self, graph: GraphInformation) -> dict:\n",
    "        \"\"\"Collects a single trajectory from the environment.\n",
    "        Returns a dictionary containing the trajectory as well as the trajectory information.\n",
    "        \"\"\"\n",
    "        self._reset_state(graph)\n",
    "        # Add one to sample [a, b] instead of [a, b)\n",
    "        rollin_length = np.random.randint(self._trajectory_length - self._rollout_length + 1)\n",
    "\n",
    "        # Roll-in phase should not contribute any gradients.\n",
    "        for i in range(rollin_length):\n",
    "            # Return an empty batch if the roll-in reaches the goal node.\n",
    "            # This should never happen as that would mean the roll-in is too long\n",
    "            # and we cannot collect any trajectories, but fail-safe nonetheless.\n",
    "            if self._run_step(rollin=True)[1]:\n",
    "                return {}\n",
    "\n",
    "        trajectory = Trajectory(\n",
    "            self._max_neighborhood_size,\n",
    "            self._controller.memory.detach().clone(),\n",
    "            device=self._device)\n",
    "\n",
    "        # Collect the trajectory\n",
    "        for steps in range(1, self._rollout_length + 1):\n",
    "            fringe_set, done = self._run_step()\n",
    "\n",
    "            # There is nothing to learn when the fringe set is empty\n",
    "            # and thus no scores to calculate.\n",
    "            if fringe_set.numel() > 0:\n",
    "                trajectory.add(fringe_set.to(self._device))\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'trajectory': trajectory,\n",
    "            'env_steps': rollin_length + steps,\n",
    "            'num_closed_nodes': len(self._env.closed_set),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c4bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Learner(Protocol):\n",
    "    def learn(self, **kwargs) -> float:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dad440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PHILLearner:\n",
    "    def __init__(self, model, params: Parameters, device='cpu'):\n",
    "        self._model = model\n",
    "        self._device = device\n",
    "        self._criterion = torch.nn.MSELoss(reduction='sum')\n",
    "        # Paper stated to use the Adam optimizer with a learning rate of 0.001\n",
    "        self._optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    def _get_values(self, graph_information: GraphInformation, trajectory: Trajectory):\n",
    "        # Clone the memory at the start of the rollout.\n",
    "        memory = trajectory.rollin_memory.detach().clone().to(self._device)\n",
    "        # Initialize full tensors as this is a bit more efficient\n",
    "        # than concatenating every iteration.\n",
    "        num_open_nodes = sum(len(fringe_set) for fringe_set in trajectory.fringe_sets)\n",
    "        values = torch.zeros([num_open_nodes], dtype=torch.float32, device=self._device)\n",
    "        node_index = torch.zeros([num_open_nodes], dtype=torch.long, device=self._device)\n",
    "        index = 0\n",
    "        # Recompute the heuristic values for every iteration.\n",
    "        for fringe_set in trajectory.fringe_sets:\n",
    "            indices = torch.arange(index, index + len(fringe_set), device=self._device)\n",
    "            node_index[indices] = fringe_set\n",
    "            heuristics, memory = self._model(\n",
    "                graph_information.graph,\n",
    "                fringe_set,\n",
    "                graph_information.goal_index,\n",
    "                memory)\n",
    "            values = torch.scatter(values, 0, indices, heuristics.squeeze(1))\n",
    "            index += len(fringe_set)\n",
    "\n",
    "        return values, node_index, num_open_nodes\n",
    "\n",
    "    def learn(self, dataset: Dataset, database: TrajectoryDatabase) -> torch.float32:\n",
    "        total_loss = 0.0\n",
    "        losses = 0\n",
    "        # Ensure model collects gradients.\n",
    "        self._model.train()\n",
    "\n",
    "        # Sample a graph and its trajectories\n",
    "        for graph_id, trajectories in database:\n",
    "            graph = sample_graph_information(dataset[graph_id], device=self._device)\n",
    "\n",
    "            # For every trajectory re-evaluate all the heuristic scores.\n",
    "            for trajectory in trajectories:\n",
    "                values, node_index, num_open_nodes = self._get_values(graph, trajectory)\n",
    "                # Grab the corresponding targets from the graph.\n",
    "                targets = graph.graph.y[node_index]\n",
    "                # Loss function from the paper\n",
    "                loss = self._criterion(targets.detach(), values) / num_open_nodes\n",
    "                self._optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # loss.backward(retain_graph=True)\n",
    "                self._optimizer.step()\n",
    "                # Discard the computation graph of the memory.\n",
    "                # memory.detach() # TODO?\n",
    "                total_loss += loss.item()\n",
    "                losses += 1\n",
    "\n",
    "        # Normalize the loss, otherwise it would scale with the trajectory count.\n",
    "        return total_loss / losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ae201",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \"\"\"Abstract class of an experiment. Contains logging and plotting functionality.\"\"\"\n",
    "    def __init__(self, model, plot_frequency=1, plot_episodes=True, **kwargs):\n",
    "        self._model = model\n",
    "\n",
    "        # Plotting\n",
    "        self._plot_frequency = plot_frequency # Plot per how many episodes\n",
    "        self._plot_episodes = plot_episodes\n",
    "        self.plot_line_color = 'b'\n",
    "\n",
    "        # Stats\n",
    "        self._episode_losses = []\n",
    "        self._episode_num_closed_nodes = []\n",
    "        self._env_steps = []\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    def plot_training(self, update=False):\n",
    "        \"\"\"Plots logged training results. Use \"update=True\" if the plot is continuously updated\n",
    "        or use \"update=False\" if this is the final call (otherwise there will be double plotting).\"\"\"\n",
    "        window = max(len(self._episode_losses) // 10, 5)\n",
    "\n",
    "        if len(self._episode_losses) < window + 2:\n",
    "            return\n",
    "\n",
    "        losses = np.convolve(self._episode_losses, np.ones(window) / window, mode='valid')\n",
    "        num_closed_nodes = np.convolve(self._episode_num_closed_nodes, np.ones(window) / window, mode='valid')\n",
    "        env_steps = np.convolve(self._env_steps, np.ones(window) / window, mode='valid')\n",
    "\n",
    "        if self._plot_episodes:\n",
    "            x_losses = [i + window for i in range(len(losses))]\n",
    "        else:\n",
    "            x_losses = env_steps[(len(env_steps) - len(losses)):]\n",
    "\n",
    "        # Get/create plot\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(12, 4)\n",
    "        plt.clf()\n",
    "\n",
    "        pl.subplot(1, 2, 1)\n",
    "        pl.plot(x_losses, losses, self.plot_line_color)\n",
    "        pl.xlabel('episodes' if self._plot_episodes else 'environment steps')\n",
    "        pl.ylabel('loss')\n",
    "        fig.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "        pl.subplot(1, 2, 2)\n",
    "        pl.plot(x_losses, num_closed_nodes, self.plot_line_color)\n",
    "        pl.xlabel('episodes' if self._plot_episodes else 'environment steps')\n",
    "        pl.ylabel('Number of nodes in the closed set')\n",
    "        fig.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "        # dynamic plot update\n",
    "        display.clear_output(wait=True)\n",
    "        if update:\n",
    "            display.display(pl.gcf())\n",
    "            \n",
    "    def close(self):\n",
    "        assert False, 'the close method should be overridden.'\n",
    "\n",
    "    def run(self):\n",
    "        assert False, 'the run method should be overridden.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ILExperiment(Experiment):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model,\n",
    "        params: Parameters,\n",
    "        device='cpu',\n",
    "        print_when_plot=True,\n",
    "        plot_episodes=True,\n",
    "        state=None,\n",
    "    ):\n",
    "        super().__init__(model, plot_episodes=plot_episodes)\n",
    "        self._learner = PHILLearner(model, params, device)\n",
    "        self._controller = OracleController(model, ['heuristic', 'oracle'], params, device)\n",
    "        self._runner = AStarRunner(self._controller, params, device)\n",
    "        self._models = []\n",
    "        self._database = TrajectoryDatabase()\n",
    "\n",
    "        # Config\n",
    "        self._print_when_plot = print_when_plot\n",
    "        self._device = device\n",
    "\n",
    "        # Hyper parameters\n",
    "        self._params = params\n",
    "        self._max_episodes = params['max_episodes']\n",
    "        self._num_trajectories = params['num_trajectories']\n",
    "        self._patience = params['patience']\n",
    "        self._patience_margin = params['patience_margin']\n",
    "\n",
    "    def save(self, path: pathlib.Path):\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        state = {\n",
    "            'episode_losses': self._episode_losses,\n",
    "            'episode_num_closed_nodes': self._episode_num_closed_nodes,\n",
    "            'env_steps': self._env_steps,\n",
    "            'models': [model.state_dict() for model in self._models],\n",
    "            'database': self._database,\n",
    "            'device': self._device,\n",
    "        }\n",
    "\n",
    "        with open(path, 'wb') as handle:\n",
    "            pickle.dump(state, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load(self, path: pathlib.Path):\n",
    "        if not path.exists():\n",
    "            return\n",
    "\n",
    "        with open(path, 'rb') as handle:\n",
    "            state = pickle.load(handle)\n",
    "\n",
    "        assert self._device == state['device'], 'Devices must be equal.'\n",
    "            \n",
    "        self._episode_losses = state['episode_losses']\n",
    "        self._episode_num_closed_nodes = state['episode_num_closed_nodes']\n",
    "        self._env_steps = state['env_steps']\n",
    "        self._database = state['database']\n",
    "\n",
    "        if len(state['models']) == 0:\n",
    "            return\n",
    "\n",
    "        self._model.load_state_dict(state['models'][-1])\n",
    "        self._learner = PHILLearner(self._model, self._params, device)\n",
    "        self._controller = OracleController(self._model, ['heuristic', 'oracle'], self._params, device)\n",
    "        self._runner = AStarRunner(self._controller, self._params, device)\n",
    "\n",
    "        for model_state in state['models']:\n",
    "            model = deepcopy(self._model)\n",
    "            model.load_state_dict(model_state)\n",
    "            self._models.append(model)\n",
    "\n",
    "    def _reset_state(self, restart=False):\n",
    "        if restart or len(self._models) == 0:\n",
    "            self._episode_losses = []\n",
    "            self._episode_num_closed_nodes = []\n",
    "            self._env_steps = []\n",
    "            self._models = []\n",
    "            self._database = TrajectoryDatabase()\n",
    "\n",
    "        # Start from the previous model.\n",
    "        if len(self._models) > 0:\n",
    "            self._model = self._models[-1]\n",
    "\n",
    "        # Initial plot after continuing the run\n",
    "        if len(self._episode_losses) > 2:\n",
    "            self._plot_and_print()\n",
    "\n",
    "    def close(self):\n",
    "        # Trim all the stats lists to be the same size\n",
    "        if self._database is not None:\n",
    "            n = len(self._models) # Models is the last to be updated, therefor our baseline\n",
    "            self._episode_losses = self._episode_losses[:n]\n",
    "            self._episode_num_closed_nodes = self._episode_num_closed_nodes[:n]\n",
    "            self._env_steps = self._env_steps[:n]\n",
    "\n",
    "    def _plot_and_print(self):\n",
    "        # Replace current output with new plot.\n",
    "        self.plot_training(update=True)\n",
    "\n",
    "        if self._print_when_plot:\n",
    "            print('Episode %u, loss %.4g, 5-epi-loss %.4g +- %.3g, length %u +- %u' % (\n",
    "                len(self._episode_losses),\n",
    "                self._episode_losses[-1],\n",
    "                np.mean(self._episode_losses[-5:]), \n",
    "                np.std(self._episode_losses[-5:]),\n",
    "                np.mean(self._episode_num_closed_nodes[-5:]),\n",
    "                np.std(self._episode_num_closed_nodes[-5:]),\n",
    "            ))\n",
    "\n",
    "    def run(self, train_dataset: Dataset, restart=False):\n",
    "        \"\"\"Starts or continues the experiment.\"\"\"\n",
    "        self._reset_state(restart)\n",
    "        env_steps = 0\n",
    "        best_loss = 1e20\n",
    "        # Random ordering of the training graphs.\n",
    "        permute_order = np.random.permutation(len(train_dataset))\n",
    "\n",
    "        # Sample an environment (graph) and collect trajectories from it.\n",
    "        # After each set of trajectories has been collected, do a gradient update step.\n",
    "        for i in range(min(len(permute_order), self._max_episodes) ):\n",
    "            graph_id = permute_order[i]\n",
    "            print(f'Working on graph: {graph_id}, training...', end='')\n",
    "            graph = sample_graph_information(train_dataset[graph_id], random=True, device=self._device)\n",
    "            self._episode_trajectories = 0\n",
    "            episode_num_closed_nodes = []\n",
    "\n",
    "            # Ensure the model collects no gradients.\n",
    "            self._model.eval()\n",
    "\n",
    "            # Collect trajectories\n",
    "            for _ in range(self._episode_trajectories, self._num_trajectories):\n",
    "                batch = self._runner.run(graph)\n",
    "\n",
    "                # Goal could be reached during roll-in, yielding no trajectory.\n",
    "                if 'trajectory' not in batch:\n",
    "                    continue\n",
    "\n",
    "                # Add the trajectory from graph i to the database.\n",
    "                self._database.add(permute_order[i], batch['trajectory'])\n",
    "                env_steps += batch['env_steps']\n",
    "                episode_num_closed_nodes.append(batch['num_closed_nodes'])\n",
    "\n",
    "            # Perform a gradient update step\n",
    "            print(', learning...')\n",
    "            loss = self._learner.learn(train_dataset, self._database)\n",
    "            # Update stats\n",
    "            self._episode_losses.append(loss)\n",
    "            self._episode_num_closed_nodes.append(np.mean(episode_num_closed_nodes))\n",
    "            self._env_steps.append(env_steps)\n",
    "            # Store the current iteration for validation.\n",
    "            self._models.append(deepcopy(self._model))\n",
    "\n",
    "            if len(self._episode_losses) % self._plot_frequency == 0:\n",
    "                self._plot_and_print()\n",
    "            \n",
    "            # Early stopping when the model stops performing better\n",
    "            if loss < best_loss - self._patience_margin:\n",
    "                best_loss = loss\n",
    "                patience_cnt = 0\n",
    "            # No early stopping at startup\n",
    "            elif len(self._episode_losses) > 20:\n",
    "                patience_cnt += 1\n",
    "                if patience_cnt == self._patience:\n",
    "                    self._plot_and_print()\n",
    "                    print(f'Stopped early after {len(self._episode_losses)} iterations.')\n",
    "                    return\n",
    "\n",
    "        self._plot_and_print()\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def get_best_model(self, validation_dataset: Dataset):\n",
    "        \"\"\"Use the validation dataset to find the best model h0_i\"\"\"\n",
    "        # TODO: Use model evaluator to evaluate models.\n",
    "        # But this takes a longgggg ass time for ~50 models and 100 validation graphs...\n",
    "        return self._models[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "delEvaluator:\n",
    "    def __init__(self, params: Parameters, device='cpu'):\n",
    "        self._params = params\n",
    "        self._device = device\n",
    "\n",
    "    def _run(self, env: AStarEnvironment, controller: Controller):\n",
    "        explored_nodes = set()\n",
    "\n",
    "        while True:\n",
    "            heuristic = controller.choose(True)\n",
    "\n",
    "            #print('Heuristic is: ', heuristic)\n",
    "            \n",
    "            try:\n",
    "                fringe_set, done = env.run(heuristic)\n",
    "                explored_nodes.update(fringe_set.tolist())\n",
    "            except KeyError:\n",
    "                return explored_nodes\n",
    "\n",
    "            if done:\n",
    "                return explored_nodes\n",
    "\n",
    "\n",
    "    def show_path(self, graph: Data, closed_nodes: set, num_explored_nodes: list[int], graph_id: int, save_figure: bool = False):\n",
    "        width = int(graph.x[:,0].max().item()) + 1\n",
    "        height = int(graph.x[:,1].max().item()) + 1\n",
    "        cmap = plt.cm.colors.ListedColormap(['black', 'white', 'blue'])\n",
    "        img = torch.zeros((height, width), dtype=torch.int8)\n",
    "        x = graph.x.to(torch.int)\n",
    "        img[x[:,1], x[:,0]] = 1\n",
    "\n",
    "        # Get the closed nodes from the PHIL A*\n",
    "        for n in closed_nodes:\n",
    "            pos_y = int(graph.x[n][0])\n",
    "            pos_x = int(graph.x[n][1])\n",
    "            img[pos_x, pos_y] = 2\n",
    "\n",
    "        # Invert the x-axis\n",
    "        # img = np.flip(img,axis=0)\n",
    "\n",
    "        # Display the matrix as an image with the defined colormap\n",
    "        plt.imshow(img.numpy(), cmap=cmap)\n",
    "        # plt.xlim([-1, 201])\n",
    "        # plt.ylim([-1, 201])\n",
    "\n",
    "        # Plot goal node\n",
    "        plt.scatter(0, width, s=50, c='red', marker='o')\n",
    "        # Source node\n",
    "        plt.scatter(height, 0, s=50, c='green', marker='o')\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "        if save_figure:\n",
    "          plt.savefig('graph_' + str(graph_id) + '.jpg')\n",
    "\n",
    "        print('Graph %u, nodes evaluated %.4g +- %.3g' % (\n",
    "            graph_id,\n",
    "            np.mean(num_explored_nodes),\n",
    "            np.std(num_explored_nodes),\n",
    "        ))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model, dataset: Dataset, show_path=True) -> list[int]:\n",
    "        \"\"\"Evaluates the model performance and returns\n",
    "        the number of nodes in the closed set for every testing graph.\"\"\"\n",
    "        num_explored_nodes = []\n",
    "\n",
    "        to_save_graphs = [3, 4, 5, 6]\n",
    "\n",
    "        # Put model in evaluation mode\n",
    "        model.eval()\n",
    "        controller = OracleController(model, ['heuristic', 'heuristic'], self._params, device)\n",
    "        env = AStarEnvironment(controller.heuristics, self._params)\n",
    "\n",
    "        for i, graph in enumerate(dataset):\n",
    "            graph_info = sample_graph_information(graph, random=False, device=self._device)\n",
    "\n",
    "            if graph_info.source_index.numel() == 0:\n",
    "                print(f'Graph {i} has no source node, skipping')\n",
    "                continue\n",
    "\n",
    "            self.show_path(graph, set([0]), num_explored_nodes, i)\n",
    "            # Reset environments for new graph\n",
    "            controller.reset(graph_info.goal_index)\n",
    "            env.reset(graph_info.graph, graph_info.source_index, graph_info.goal_index)\n",
    "            # Run the environment\n",
    "            explored_nodes = self._run(env, controller)\n",
    "            num_explored_nodes.append(len(explored_nodes))\n",
    "\n",
    "            if show_path:\n",
    "                if i in to_save_graphs:\n",
    "                    self.show_path(graph, explored_nodes, num_explored_nodes, i, True)\n",
    "                else:\n",
    "                    self.show_path(graph, explored_nodes, num_explored_nodes, i)  \n",
    "\n",
    "        return num_explored_nodes\n",
    "\n",
    "    def _get_open_neighbors(self, graph: Data, node_index: int, closed_set: list) -> torch.LongTensor:\n",
    "        neighborhood_index = k_hop_subgraph(node_index, 1, graph.edge_index)[0]\n",
    "        # Select the open nodes from the neighborhood\n",
    "        fringe_set_mask = [neighbor.item() not in closed_set\n",
    "                           for neighbor in neighborhood_index]\n",
    "        fringe_set = neighborhood_index[fringe_set_mask]\n",
    "        return fringe_set\n",
    "\n",
    "    def evaluate_a_star(self, dataset: Dataset, show_path=True) -> list[int]:\n",
    "        \"\"\"Evaluates the model performance with an aribtrary function\n",
    "\n",
    "        Arbitrary function needs to receive two values as input, an x and y\n",
    "        node\n",
    "        \n",
    "        Is it a crap implementation? Yes\n",
    "        Does it work? Hopefully!\"\"\"\n",
    "        num_explored_nodes = []\n",
    "\n",
    "        to_save_graphs = [3, 4, 5, 6]\n",
    "\n",
    "        # controller = OracleController(model, ['heuristic', 'heuristic'], self._params, device)\n",
    "        # env = AStarEnvironment(controller.heuristics, self._params)\n",
    "\n",
    "        for i, graph in enumerate(dataset):\n",
    "            graph_info = sample_graph_information(graph, random=False, device=self._device)\n",
    "            if graph_info.source_index.numel() == 0:\n",
    "                print(f'Graph {i} has no source node, skipping')\n",
    "                continue\n",
    "\n",
    "            self.show_path(graph, set([0]), num_explored_nodes, i)\n",
    "\n",
    "            #open_set = [graph_info.source_index]\n",
    "            explored_nodes = []\n",
    "            goal_index = graph_info.goal_index\n",
    "            g_scores = {}\n",
    "\n",
    "            # f_scores also represents the open set\n",
    "            f_scores = {graph_info.source_index.item(): 0}\n",
    "\n",
    "            # print('Source is: ', graph_info.source_index)\n",
    "            # print('Source (item) is: ', graph_info.source_index.item())\n",
    "\n",
    "            #current = graph_info.source_index\n",
    "\n",
    "            while len(f_scores.keys()) > 0:\n",
    "              current = min(f_scores, key=f_scores.get)\n",
    "              # Deleting the entry of f_scores\n",
    "              f_scores.pop(current, None)\n",
    "              explored_nodes.append(current)\n",
    "\n",
    "              if current == goal_index:\n",
    "                num_explored_nodes.append(len(explored_nodes))\n",
    "                break\n",
    "\n",
    "              #print('Current is: ',current)\n",
    "\n",
    "              neighbors = self._get_open_neighbors(graph, current, explored_nodes)\n",
    "\n",
    "              for index in neighbors:\n",
    "                #print('Neighbor is: ',index)\n",
    "                # Euclidean distance seems most popular for distance between nodes.\n",
    "                dist = torch.norm(graph.x[current] - graph.x[index], 2).item()\n",
    "\n",
    "                hash_code = index.item()\n",
    "                \n",
    "                if hash_code in g_scores:\n",
    "                  temp_g_score = g_scores[hash_code] + dist\n",
    "                else:\n",
    "                  temp_g_score = 0\n",
    "\n",
    "                if hash_code not in g_scores or temp_g_score < g_scores[hash_code]:\n",
    "                  g_scores[hash_code] = temp_g_score\n",
    "\n",
    "                  f_scores[hash_code] = temp_g_score + torch.norm(graph.x[hash_code] - graph.x[goal_index], 2).item()\n",
    "\n",
    "            if show_path:\n",
    "                if i in to_save_graphs:\n",
    "                    self.show_path(graph, explored_nodes, num_explored_nodes, i, True)\n",
    "                else:\n",
    "                    self.show_path(graph, explored_nodes, num_explored_nodes, i)  \n",
    "\n",
    "        return num_explored_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c24bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing this code-block defines a new experiment\n",
    "def new_experiment(dataset: Dataset, device='cpu') -> ILExperiment:\n",
    "    params = default_parameters()\n",
    "    model = HeuristicFunction(\n",
    "        num_node_features=dataset.num_node_features,\n",
    "        params=params,\n",
    "    ).to(device)\n",
    "    return ILExperiment(model, params, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bf373",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64365eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alternating_gaps_experiment_path = experiment_path / f'alternating_gaps_{device}.pickle'\n",
    "alternating_gaps_experiment = new_experiment(alternating_gaps_train, device)\n",
    "\n",
    "if alternating_gaps_experiment_path.exists():\n",
    "    alternating_gaps_experiment.load(alternating_gaps_experiment_path)\n",
    "\n",
    "# Re-executing this code-block picks up the experiment where you left off\n",
    "try:\n",
    "    alternating_gaps_experiment.run(alternating_gaps_train)\n",
    "    alternating_gaps_experiment.save(alternating_gaps_experiment_path)\n",
    "except KeyboardInterrupt:\n",
    "    alternating_gaps_experiment.close()\n",
    "    alternating_gaps_experiment.plot_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segundo_entorno",
   "language": "python",
   "name": "nombre_del_nuevo_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
